\def\year{2021}\relax

%File: formatting-instructions-latex-2021.tex
%release 2021.2
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS

% TODO: remove
% \newcommand{\development} % dev / arxiv version

\usepackage{aaai21}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%\nocopyright
%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.
\pdfinfo{
% /Title (AAAI Press Formatting Instructions for Authors Using LaTeX -- A Guide)
/Author (AAAI Press Staff, Pater Patel Schneider, Sunil Issar, J. Scott Penberthy, George Ferguson, Hans Guesgen, Francisco Cruz, Marc Pujol-Gonzalez)
/TemplateVersion (2021.2)
} %Leave this
% /Title ()

% Put your actual complete title (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case
% Leave the space between \Title and the beginning parenthesis alone
% /Author ()
% Put your actual complete list of authors (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case.
% Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands,
% remove them.

% \JK{TODO: check added packages}
% Added packages follows:
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{newfloat}
\usepackage[framemethod=TikZ]{mdframed}


% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai21.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash

% for comments only
\usepackage{color}
\usepackage{xcolor}
%\usepackage{bera}% optional: just to have a nice mono-spaced font
%\usepackage[scaled]{beramono} 
% OD: ^-- this would be the correct command (monospaced only, keep rest intact), 
%         but the instructions say we should use Courier, so I'd rather not tamper with it.
\usepackage{listings}
\newenvironment{customBoxed}
 {\trivlist\nopagebreak
  \parindent0pt
  \item\relax\obeylines}
 {\par
  \nopagebreak
  \vspace{0.2em}%
  \endtrivlist}



\newcommand{\customComment}[2]{
	\begin{customBoxed}
		\color{#2}
		\fbox{\parbox{\linewidth}{#1}}
	\end{customBoxed}}
\newcommand{\JK}[1]{\customComment{#1}{olive}\PackageWarning{paper}{JK: #1}}
\newcommand{\OD}[1]{\customComment{#1}{cyan!89!yellow!80!black!100}\PackageWarning{paper}{OD: #1}}
\newcommand{\VH}[1]{{\textcolor{green!100!yellow!70!black!100!}{VH: #1}}}
\newcommand{\TN}[1]{\customComment{#1}{magenta}\PackageWarning{paper}{TN: #1}}
\newcommand{\todo}[1]{{\textcolor{red}{TODO: #1}}}
\newcommand{\multiwoz}[0]{MultiWOZ 2.0 }
\newcommand{\multiwozn}[0]{MultiWOZ 2.1 }
\newcommand{\taskmaster}[0]{Taskmaster-1 }
\newcommand{\schema}[0]{Schema-Guided Dialogue }
\newcommand{\code}[1]{\texttt{#1}}

\mdfdefinestyle{ExampleFrame}{%
    nobreak=true,
    innertopmargin=4pt,
    innerbottommargin=4pt,
    innerrightmargin=4pt,
    innerleftmargin=4pt,
    outermargin=0,
    }
% \DeclareFloatingEnvironment[placement={!ht},name=Example]{example}
%\captionsetup[example]{aboveskip=2pt}%,labelfont=bf}

\ifdefined\development
    \usepackage{hyperref}
    \usepackage{microtype}  % OD added: better typography, uses slightly less space than original
    \DeclareFloatingEnvironment[placement={!ht},name=Example]{example}
    % JK: to use the same flow as for the submission
    % JK: TODO: remove for better text flow
    % \captionsetup[example]{aboveskip=2pt}%,labelfont=bf}
    \newcommand{\exampleref}[1]{Example~\ref{#1}}
    
    \thispagestyle{plain} % XXX remove this
\pagestyle{plain} % XXX remove this
\renewcommand{\thepage}{\textcolor{red}{\rule{0pt}{5ex}\small Page {\bf\Large\arabic{page}} (remove page numbers before submitting!)}} % XXX remove this

\else
    \newenvironment{example}{\begin{figure}[!ht]}{\end{figure}}
    \newcommand{\exampleref}[1]{Figure~\ref{#1}}
\fi


% end comments

%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{AuGPT: Dialogue with Pre-trained Language Models and Data Augmentation}

\author {
    % Authors
    Anonymous \\
    %Jonáš Kulhánek,\textsuperscript{\rm 1,\rm 2}
    %Vojtěch Hudeček, \textsuperscript{\rm 1}
    %Tomáš Nekvinda, \textsuperscript{\rm 1}
    %Ondřej Dušek\textsuperscript{\rm 1} \\
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1} Affiliation 1 \\
    \textsuperscript{\rm 2} Affiliation 2 \\
    %jonas.kulhanek@cvut.cz, \{hudecek,nekvinda,odusek\}@ufal.mff.cuni.cz
    emails
}

\begin{document}
\maketitle

\begin{abstract}
Attention-based pre-trained language models such as GPT-2 have revolutionized the field of NLP. Dialogue modeling has also benefited from their progress. However, they also present considerable risks for task-oriented dialogue, such as lack of knowledge grounding or diversity. To address these issues, we introduce modified training objectives for language model finetuning, and we employ massive data augmentation via back-translation to increase the diversity of the training data. We further examine the possibilities of combining data from multiples sources to improve performance on the target dataset. We carefully evaluate our contributions with both human and automatic methods. Our model achieves state-of-the-art performance on the MultiWOZ data and is competitive with the DSTC9 shared task's best submissions.
\end{abstract}
\section{Introduction}
Unlike traditional task-oriented systems based on modularized pipelines \cite{young2013, gao2018}, end-to-end dialogue systems integrate nearly all functionality required to hold a dialogue into a single neural network \cite{wen2017,manning2017,lei2018}, reducing error-propagation and data annotation requirements. While these systems are not yet ready for production use, they made considerable progress in recent years, especially with the advent of pre-trained neural language models (LMs) \cite{devlin2019,radford2019,zhang2020dialogpt}.
Systems such as GPT-2 finetuned by \citet{budzianowski2019} show that with an LM pre-trained on a large number of general-domain dialogues without annotation, only small amounts of data are required to perform in a given task-oriented domain.

The pre-trained LMs still run enormous risks. First, solely training for response generation may result in a lack of grounding for the responses, where the LM hallucinates words without any relation to the database. This has been addressed by multi-task training, and auxiliary training objectives \cite{peng2020} to an extent. Second, finetuning on small datasets may reduce response diversity and fluency due to neural networks' known propensity for catastrophic forgetting -- the model overfits the finetuning set too tightly.

This paper presents AuGPT -- our entry to the DSTC9 Shared Task Track 2 \cite{gunasekara2020overview} for multi-domain task-oriented response generation on the MultiWOZ data \cite{budzianowski2018},\footnote{\url{https://convlab.github.io/}} where we address the above problems with pre-trained LMs. AuGPT is based on the DialoGPT pre-trained LM. To improve response consistency, we build upon \citet{peng2020}'s auxiliary training objectives and introduce improvements. For higher output diversity and fluency, we apply massive training data augmentation through additional task-oriented datasets and back-translation \cite{sennrich2016}. Our system placed 3rd out of 10 in the overall DSTC~9 Track~2 systems ranking, scoring best overall on dialogue success when database grounding is not considered while providing the most efficient (shortest) dialogues overall.

Our contributions can be summarized as follows:
\begin{itemize}
    \item We show that augmenting data using back-translation markedly improves performance in task-oriented settings.
    \item We introduce an improved dialogue consistency classification task used as an auxiliary training objective, based on subtle changes to the dialogue state (instead of replacing the state with a random one). This brings additional performance improvement.
    \item To further improve output diversity, we experiment with unlikelihood loss \cite{welleck2019,li_dont_2020}.
    \item In addition to the official DSTC9 evaluation, we provide results for our model and comparisons to multiple baselines on MultiWOZ versions 2.0 and 2.1. To our knowledge, our system obtains state-of-the-art results for an end-to-end model on this dataset.
\end{itemize}

To advocate research transparency and reproducibility, we publish our augmented training data, source code, and pre-trained models on GitHub.\footnote{Link removed to preserve anonymity; code will be released with the final version of this paper.}

\section{Related Work}
While the first attempts to build generative end-to-end task-oriented systems mimicked the traditional dialogue system components \citep{wen2017}, the task was soon recast as a sequence prediction problem in a two-stage setup. A sequence-to-sequence (seq2seq) model first generates the belief state based on dialogue context, then generates the system response based on the context and the belief state \cite[Sequicity;][]{lei2018}.
The basic recurrent seq2seq architecture \cite{sutskever2014} was enhanced with various attention mechanisms to support copying tokens from the context into the belief state or from the belief state onto the output \cite{wu2018, shu2019}. Database operations typically stay outside the model, but some approaches even include the database in the model via attention over possible entries \cite{manning2017,wen2018,madotto2018}. 

Recently, large-scale task-oriented datasets were proposed \cite{budzianowski2018, byrne2019, rastogi2019} to encourage research in the field of multi-domain dialogue modeling. 
Even though the aforementioned approaches achieved promising results on smaller domains such as CamRest676 \cite{wen2016}, their ability to generalize over multiple domains remains limited \cite{zhang2019}. 
%To tackle this issue, \citet{mehri2019} propose a return to a modular network with pre-trained component networks, \citet{madotto2020} suggest meta-learning for domain-aware parameter selection.
To address this, \citet{zhang2020end2end} introduce the LABES-S2S model that -- in addition to a two-stage seq2seq approach -- models belief states as discrete latent variables.
\citet{zhang2019} present DAMD, a three-stage seq2seq architecture which explicitly decodes the system action. They optimize for multiple good actions given a single belief state.
% \citet{mehri2019}  present a pre-trained and fine-tuned modular system and \citet{madotto2020} suggest meta-learning for domain-aware parameter selection.

The line of research that is closest to our work makes use of large pre-trained LMs 
based on the transformer architecture \cite{vaswani2017} such as GPT-2 \cite{radford2019} or BERT \cite{devlin2019}. 
Finetuned variants of these models achieved state-of-the-art results on many NLP tasks, e.g., question answering or natural language inference \cite{liu2020},  
and they seem to be useful even in the context of dialogue systems. 
For example, \citet{wu2020} % adapted the objective function of the original BERT model and 
propose finetuning BERT \cite{devlin2019} for task-oriented dialogue on multiple datasets; 
\citet{zhang2020dialogpt} extended the GPT-2 LM to model open-domain chit-chat.

We follow research initiated by \citet{budzianowski2019}, who use GPT-2 to model multi-domain task-oriented dialogues.  
Recently, three similar modifications to the model were proposed, namely SOLOIST \cite{peng2020}, SimpleTOD \cite{hosseini2020}, and the approach by \citet{ham2020}. 
This work further develops these models and proposes a novel training approach and data augmentation strategies based on back-translation \cite{edunov2018,federmann2019multilingual}.

\section{Method}
\begin{figure*}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{schema_v2} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{The architecture of \textbf{AuGPT}. The pipeline runs in two stages. First, a finetuned GPT-2 LM is used to predict a belief. Then the database results are obtained and everything is passed to the GPT-2 again to predict a final delexicalized response, along with possible auxiliary tasks (belief consistency, intent classification, system action classification). Unlikelihood loss is used for response prediction training.}
\label{fig:pipeline}
\end{figure*}

The task-oriented setting requires the dialogue system to respond adequately to the user's input and fulfill its goal. The goal could be, e.g., \ booking a train or requesting restaurant details. To achieve that, the system has to process the user's input, keep track of the belief state with respect to user preferences regarding individual in-domain attributes (slots) and generate a relevant response in natural language. The system also must be able to interact with an external database to incorporate the necessary information into the generated response (see Figure~\ref{fig:pipeline} for an example).

Due to its excellent language modeling and language generation capabilities, we have chosen the pre-trained GPT-2 LM as our system's backbone architecture. Similarly to \citet{budzianowski2019}, we use the LM to model both the belief state and the response.

\subsection{Model Representation}

The training instances for an LM-based task-oriented dialogue system can be considered as tuples $(c, b, d, r)$, where $r$ is the system's response, $c$ is the context (i.e., a concatenation of all previous utterances in the dialogue – both system's and user's), $b$ is the system's belief state which is also used for querying the database, and $d$ are the database results. 

In our case, the dialogue system handles multiple domains and the belief state is a set of pairs (\emph{domain name}, \emph{domain belief}), where the \emph{domain belief} is an assignment of values into slots, i.e., a set of pairs (\textit{slot name}, \textit{value}) (see \exampleref{ex:augpt_format}). Similarly, the database results $d$ are a set of pairs (\textit{domain name}, \textit{domain database results}), where the \textit{domain database results} are an ordered list of entities returned by the database. We further define the \emph{database result counts} $d_c$ denoting the number of results in $d$ for each domain.

Ideally, we would like our system to model the probability distribution over possible responses conditioned on the context $p(r|c)$. To simplify computation and model the interaction with an external database, this distribution can be factorized as follows:
\begin{equation}
\begin{split}
    p(r|c) &= \sum_{d} p(r|d,c) p(d|c) \\
           &= \sum_{d} \sum_{b} p(r|d,b,c) p(d|b) p(b|c) \\
           &= \sum_{b} p(r|\textit{Query}(b),b,c) p(b|c)\,,
\end{split}
\end{equation}
where $p(d|b)$ is a deterministic distribution over the database results, and \textit{Query} is a function returning database results.

By using this formulation and by modeling $p(r|d,b,c)$ and $p(b|c)$, our model would be able to process the context, query the database, and generate the response based on the database results. However, we would face a problem with data sparsity when estimating parameters of $p(r|d,b,c)$. The reason for the data sparsity is the relatively small size of the dataset and the responses containing underrepresented, sometimes unique words, such as reference numbers, hotel names, etc. To maximally reuse the training samples, we choose to train our model on \emph{delexicalized responses} \cite{wen2015} denoted $\bar{r}$. During inference, the responses are lexicalized back deterministically using both the belief state and the database results. We assume perfect lexicalization, i.e., we are able to lexicalize the response $\bar{r}$ back based on $d$ and $b$. This holds for almost all responses in the dataset.


Both the database lookup and the lexicalization are deterministic, and the delexicalized response $\bar{r}$ does not depend on the database results $d$, but only on their counts $d_c$. Therefore, the distribution $p(r|d,b,c)$ is equal to the distribution $p(\bar{r}|d_c,b,c)$, and by maximizing its likelihood we are achieving the goal of maximizing the likelihood of $p(r|c)$.

We use the same language model $\hat{p}$ to model the belief state and to generate the delexicalized prediction. That is,
\begin{equation}
    p(\bar{r}|d_c,b,c) \approx \hat{p}(\bar{r}|d_c,b,c,\theta)
\end{equation}
\begin{equation}
    p(b|c) \approx \hat{p}(b|\emptyset,\emptyset,c,\theta)\,,
\end{equation}
where we denote the model's parameters as $\theta$.

In the MultiWOZ dataset \cite{budzianowski2018,eric2019}, the responses are delexicalized by replacing concrete values with placeholder tokens of the form \textit{domain\_slot}. For better generalization across domains, we chose to use only \textit{slot} instead. We had noticed it was never the case that a response would involve more than one domain. Therefore, we decided to train our model to detect the \textit{active domain} and used the predicted active domain during the final lexicalization. The model predicts the active domain by outputting it as the first domain in the belief state. The other domains then follow in lexicographical order. The disadvantage of this approach is that we cannot determine the active domain if the belief state is empty. However, in such a case the lexicalization would fail anyway, so the system's performance is not affected by this decision.

\begin{example}
\begin{mdframed}[style=ExampleFrame]
%\begin{lstlisting}[columns=fullflexible,breaklines=true,breakindent=20pt,frame=tlrb,framextopmargin=0pt,framexbottommargin=0pt,framesep=0pt]
Belief state: train=\{ leave at=15:30, arrive by=17:15 \}, \\
\-\hspace{10pt}hotel = \{ price range = cheap \} \\
DB: train 23 matches, hotel no match
%\end{lstlisting}
\end{mdframed}
\caption{String format for AuGPT's belief state and database result count\label{ex:augpt_format}}
\end{example}

To generate the belief state and to input the database result counts to our model, we need a string representation. To fully exploit pre-training on natural language texts, we have chosen a compact representation containing as few special tokens as possible (see \exampleref{ex:augpt_format}).


\subsection{Model Training}
Although the parameters are shared for the belief state predictor and the delexicalized response predictor, the training objectives slightly differ. We use the cross-entropy loss for both predictions. For the response prediction, the unlikelihood loss \cite{welleck2019,li_dont_2020} is used as an additional objective. The unlikelihood loss gives a penalty for each repeated token, which helps the model avoid repetitions and makes frequent words less likely, increasing the answers' diversity.

To help the model learn a better internal representation from the data, we employ additional auxiliary tasks. Similarly to \citet{devlin2019} and \citet{peng2020}, we train a binary classifier to detect dialogue inconsistencies. In each training batch, we corrupt half of the samples by randomly applying one or more of the following changes with the same probability:
\begin{enumerate}
    \item We replace the belief state $b$ with another belief state, sampled uniformly randomly from the training data.
    \item We replace the delexicalized response $\bar{r}$ with a different randomly chosen one. If this change is applied in combination with the first one, the delexicalized response and the belief state are taken from the same random sample.
    \item A different valid value is uniformly sampled for each slot in the belief state. In this case, the domains' names are kept unchanged, including the order of domains -- the active domain is the same.
\end{enumerate}
The first two changes are the same as those applied by \citet{peng2020}, whereas the third one is a new one which we find very useful in the context of multiple domains, where it is much more challenging to detect if the belief state was changed when the domain names are kept the same.
The consistency detection binary classifier is trained to recognize negative samples from the positive ones based on the last response token features. It is represented by an affine classifier trained using binary cross-entropy (BCE).

We also experiment with additional two classifiers predicting the user intent and the system action. These are implemented as two fully-connected layers attached to the feature representations of the last context token and the last database result token, respectively. 
However, based on our experimental results, we decided not to use these tasks in the final model.

We train the whole pipeline by optimizing the non-weighted sum of individual component losses, i.e., cross-entropy for the belief state and the response prediction, unlikelihood loss for the response, and BCE for the consistency detection are summed in the final \textbf{AuGPT} system.

\subsection{Response Generation}
For each user input, the system transitions through several stages before the final response is generated. First, only the previous dialogue context is passed to the LM, which greedily generates the string representation of the belief state. The belief state is then parsed and passed to the database handler. The database handler then constructs a query and returns a set of results for each domain. We take the number of results for each domain and generate the string representation of database result counts (see \exampleref{ex:augpt_format}). All strings are concatenated and again passed to the language model. This time, we utilize the nucleus sampling \cite{holtzman2019} to generate the delexicalized response. We found nucleus sampling useful for generating the response since it increases diversity, but we prefer greedy decoding for the belief state with a fixed structure. Finally, the tokens in the delexicalized response are substituted by values from the database results and the belief state. The process is illustrated in Figure~\ref{fig:pipeline}.

\subsection{Data Augmentation}
Following its successful usage in other NLP tasks, \cite{konstas_neural_2017,elder_shape_2020}, we experiment with data augmentation using paraphrases, i.e., variants of the original training utterances with different surface forms.
In our setup, we generate multiple paraphrases for each training utterance and use them to augment the training data.
This way, we effectively increase the variability of the training examples.

Generating paraphrases is not a trivial process.
Various data-driven approaches were proposed, the majority of them corpora-based \cite{madnani2010generating}.
Recently, machine translation systems proved strong performance in generating paraphrases using the back-translation procedure \cite{sennrich2016,edunov2018,federmann2019multilingual}.
We take advantage of these findings and use a trained multilingual machine translation model \cite{machavcek2020elitr,edunov2018} to paraphrase our data.
We employ ten intermediate languages and thus obtain a set of different paraphrases for each input utterance.
When training, we choose the input user utterance uniformly at random from the set of all variants of the utterance including the original one.

\section{Experiments}
\begin{table*}[htbp]
    \centering
    \begin{tabular}{l|ccc|ccc}
      \toprule
      & \multicolumn{3}{c|}{MultiWOZ 2.0} & \multicolumn{3}{c}{MultiWOZ 2.1} \\
      method & inform & success & BLEU & inform & success & BLEU \\
      \midrule
      Human & 91.0 & 82.7 & -- & 86.3 & 79.1 & -- \\
      \midrule
      \textbf{AuGPT} & 90.2 & 75.5 & 17.2 & 91.4 & 72.9 & 17.2 \\
      SOLOIST \cite{peng2020} & 85.5 & 72.9 & 16.5 & -- & -- & -- \\
      SimpleTOD \cite{hosseini2020} & 84.4 & 70.1 & 15.1 & 85.0 & 70.5 & 15.2 \\
      LABES-S2S \cite{zhang2020end2end} & -- & -- & -- & 78.1 & 67.1 & 18.3 \\
      DAMD \cite{zhang2019} & 76.3 & 60.4 & 18.6 & -- & -- & -- \\
      MD-Sequicity \cite{zhang2019} & 86.6 & 71.6 & 16.8 & -- & -- & -- \\
      \bottomrule
  \end{tabular}
  \caption{Comparison with previous works on the MultiWOZ dataset (see the Corpus-based Evaluation section for a description of the metrics). \emph{MD-Sequicity} is a variant of \citet{lei2018}'s model, extended for a multi-domain setting.}
  \label{tab:multiwoz_sota_comparison}
\end{table*}
\begin{table*}[htbp]
    \centering
    \begin{tabular}{l|ccc|ccc|cc}
      \toprule
       &  & & & \multicolumn{3}{c|}{inform} & \multicolumn{2}{c}{turn} \\
      method & complete & success & book & P & R & F1 & succ & all \\
      \midrule
      \textbf{AuGPT} & 89.4 & 60.1 & 85.7 & 64.5 & 82.1 & 70.3 & 12.7 & 14.6 \\
      DAMD \cite{zhang2019} & 39.5 & 34.3 & 51.4 & 60.4 & 59.8 & 56.3 & 15.8 & 29.8\\
      Sequicity \cite{lei2018} & 23.1 & \phantom{0}9.8 & \phantom{0}4.1 & 33.0 & 32.7 & 29.9 & 12.2 & 32.6 \\
      \bottomrule
  \end{tabular}
  \caption{ConvLab evaluation comparison with other works (see the ConvLab Evaluation section for a description of the metrics).}
  \label{tab:multiwoz_convlab_comparison}
\end{table*}


We consider a series of experiments to compare out model to current state-of-the-art methods.
We also carefully evaluate all proposed contributions through a series of ablation experiments.
Moreover, thanks to participation in the DSTC9 challenge, we have obtained human evaluation results.

\subsection{Datasets}
We have used several datasets for training our system and for the final evaluation and comparison. 

\multiwozn \cite{eric2019} is used as primary to evaluate our experiments as this was the dataset selected for the DSTC9 Task 2 challenge.
\multiwozn is an enhanced version of \multiwoz \cite{budzianowski2018} that reduces the amount of noise in the data; we also use the 2.0 version in additional experiments so that we can compare to previous works.
The dataset contains 7 distinct domains (all related to tourist information, e.g., hotels, restaurants) and 10,438 dialogues, 7,032 of which are multi-domain.

%The \multiwozn contains dialogue state labels as well as annotation of dialogue acts and booking information.

We experiment with pre-training our model on additional datasets.
For the pre-training phase, we use \taskmaster \cite{byrne2019} and \schema \cite{rastogi2019}.
Both \taskmaster and \schema are multi-domain, task-oriented, large dialogue corpora consisting of 12,215 and 22,825 dialogues, respectively.
\taskmaster was obtained using the Wizard-of-Oz and self-dialogue methods, while the collection of \schema is somewhat artificial -- humans are only employed to paraphrase machine-generated utterances.


\subsection{Data Preprocessing}\label{sec:preprocessing}

Although the MultiWOZ 2.1 dataset was collected by humans, it contains a lot of inconsistencies. We hypothesize that when using only \textit{clean} samples which are consistent with the database, the benefit of using higher quality training data outweighs the decrease in the number of training samples. This claim is further supported by experiments (see the Ablation section). To filter the training data, we choose only those dialogues where the annotated dialogue goal corresponds with the turn-level annotated data. When using the \emph{clean} samples, we omit about 30\% of the training data.


To effectively combine all our datasets, we unified the domain-slot pairs in the belief states and the delexicalization.
%
However, the datasets use different naming conventions (e.g., \code{leaveAt} vs. \code{leave\_at}) and entirely different domain and slot names even though the corresponding domain-slot pairs describe the same events (e.g., \code{restaurant-food} vs. \code{restaurant-type}). To tackle this mismatch, we created a new unified ontology and manually designed a mapping between slot names. Notably, we decided to rename some slots so they use natural language tokens, as we base our model on the GPT-2 LM which is pre-trained on natural language texts (e.g. ``\code{leaveAt}''~$\rightarrow$~``\code{leave at}'').
Our final ontology that unifies all three datasets contains 22 domains and 135 slots. % distinct slots. 
%reduces the total number of domains from 76 to 22. 

We use our own implementation of delexicalization, which translates directly to our belief state string representation -- it removes the domain from the tokens and translates the tokens into natural language. 


\subsection{Training Details}

We implement our model in the PyTorch framework \cite{pytorch}.
The model extends the \emph{small} variant of the GPT-2 model.
It consists of 12 transformer blocks with a model layer size equal to 768, having 124 million parameters in total.
For all auxiliary tasks, we use a dropout of 0.1 with label smoothing 0.1.
We use the AdamW optimizer \cite{loshchilov2017decoupled}.
For greater training effectiveness, we employ mixed-precision training \cite{micikevicius2017} through PyTorch AMP. 
The finetuning runs for 8 epochs on the \multiwozn data when all the training examples are used, and for the corresponding number of minibatches if a lower number of samples is used when using only \emph{clean} samples.
The training takes less than one day when using 4 GPUs.

\subsection{Corpus-based Evaluation}

To compare with previous results on MultiWOZ, we evaluate the model performance with a set of corpus-based intrinsic metrics on both versions of the data.
In the case of MultiWOZ 2.0, we use the original delexicalization used also by other compared methods \cite{peng2020,hosseini2020,zhang2019}. For MultiWOZ 2.1, we used our own delexicalization.
We employ the original evaluation scheme by \citet{budzianowski2018}, which provides two metrics -- the \emph{inform rate} and the \emph{success rate}. The \emph{inform rate} is the percentage of dialogues in which the system provided an appropriate entity, whereas the \emph{success rate} is the percentage of dialogues in which the system outputted all the requested information. Additionally, we compute the BLEU score \cite{papineni2002} between the generated system utterances and the ground truth to get an approximation of the output fluency.
Note that both the \emph{inform rate} and the \emph{success rate} are unaffected by using a different delexicalization and these metrics can be directly compared to other methods. A different delexicalization could, however, render a slightly different BLEU, but based on preliminary results, we believe this change has almost no effect.


\subsection{ConvLab~2 Evaluation}

The DSTC9 used the ConvLab~2 platform \cite{zhu2020} for evaluation, which includes an agent-based evaluation component.
We run the evaluation component 1,000 times, i.e.\ on 1,000 simulated conversations.
The agent mimics user behavior, interacts with the system under evaluation, and computes multiple metrics, among which the most relevant are \emph{complete}, \emph{success} and \emph{book} rates.
% https://github.com/thu-coai/ConvLab-2/blob/master/convlab2/policy/rule/multiwoz/policy_agenda_multiwoz.py
The \emph{complete rate} reflects the ratio of dialogues that are completed, i.e. all the user requests have been met.
% https://github.com/thu-coai/ConvLab-2/blob/master/convlab2/evaluator/multiwoz_eval.py
The \emph{success rate} computes the percentage of dialogues which are successful, meaning the system captures correct informed entities and provides a valid booking if requested.
Finally, the \emph{book rate} is the proportion of dialogues where the system was able to book the correct entity (hotel, restaurant, train) if it was asked to.
We also compute \emph{precision, recall} and \emph{F1 score} for the informed entities and the average number of turns in the dialogue.


\subsection{Human Evaluation}

Thanks to our participation in the DSTC9 shared task, the best one of our submissions was evaluated by human judges on the Amazon Mechanical Turk platform. The judges communicated with the agent in natural language and rated the system afterward with respect to the success/failure of the dialogue, language understanding score, and response appropriateness. Information provided by the system was additionally checked for consistency with the database, and the average of success rates given by the judges and by database grounding is used as the main metric.
We refer to \citet{gunasekara2020overview} for more details.

\begin{table*}[ht]
    \centering
    \begin{tabular}{l|ccc|ccc|ccc|cc}
      \toprule
        & \multicolumn{3}{c|}{MultiWOZ 2.1} & \multicolumn{8}{c}{ConvLab 2}  \\
       & \multicolumn{3}{c|}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{inform} & \multicolumn{2}{c}{turn} \\
      method \text& inform & success & BLEU & \hspace{-1mm}complete\hspace{-1mm} & \hspace{-1mm}success\hspace{-1mm} & book & P & R & F1 & succ & all \\
      \midrule
      \textbf{AuGPT*} & 91.4 & \textbf{72.9} & 17.2 & \textbf{89.4} & \textbf{60.1} & 85.7 & 64.5 & \textbf{82.1} & 70.3 & 12.7 & 14.6 \\
    \midrule
    w/o. unlikelihood* & 90.8 & 70.4 & 16.9 & 89.2 & 59.3 & \textbf{90.8} & 63.9 & 81.6 & 69.5 & 12.8 & 14.6 \\
    w/o. clean & \textbf{91.6} & 70.7 & 15.8 & 85.0 & 57.7 & 85.6 & 65.6  & 79.1 & 69.6 & 12.7 & 14.5 \\
    w/o. unlikelihood, clean*& 90.4 & 72.7 & \textbf{17.5} & 85.9 & 58.4 & 81.3 & 62.2 & 79.8 & 67.5 & 12.6 & \textbf{14.1} \\
    w. all auxiliary* & 91.1 & 71.4 & 16.8 & 88.7 & 59.2 & 86.0 & 64.6 & 81.1 & 69.9 & \textbf{12.6} & 14.4 \\
    \midrule
    w/o. pre-training*\textdagger & 90.7 & 67.9 & 15.1 & 88.1 & 59.8 & 83.7 & \textbf{68.1} & 80.9 & 72.1 & 13.5 & 15.6 \\
    % base & 91.0 & 75.5 & 16.73 & 84.8 & 58.2 & 84.7 & 62.2 & 79.3 & 67.3 & 13.0 & 15.3\\
    % clean & 90.3 & 68.7 & 16.50 & 87.6 & 58.8 & 85.3 & 63.7 & 79.9 & 68.7 & 13.2 & 15.3 \\
    % unlike & 90.7 & 74.6 & 15.52 & 86.3 & 59.1 & 81.4 & 67.3 & 80.1 & 71.1 & 13.0 & 14.8 \\
    % rainbow & 91.2 & 72.5 & 15.52 & 86.4 & 59.9 & 88.1 & 66.9 & 80.5 & 70.9 & 13.3 & 15.3 \\
    % \midrule
    w/o. back-translations & 89.1 & 67.9 & 15.2 & 88.9 & 58.2 & 87.4 & 68.0 & 81.6 & \textbf{72.2} & 12.9 & 14.9 \\
    % clean-pre-noback & 89.7 & 70.3 & 16.5 & 85.8 & 57.4 & 79.8 & 65.5 & 80.1 & 70.1 & 13.0 & 15.0 \\
    % base-pre-noback & 90.2 & 68.4 & 16.6 & 82.0 & 56.9 & 70.6 & 63.7 & 79.0 & 68.4 & 13.2 & 14.7 \\
    % rainbow-pre-noback & 88.8 & 68.1 & 16.8 & 86.9 & 58.1 & 84.2 & 64.6 & 80.8 & 69.6 & 12.8 & 14.8 \\
    w. old consistency & 90.7 & 71.8 & 17.0 & 85.5 & 57.8 & 86.0 & 65.2 & 80.0 & 69.8 & 12.7 & 14.6  \\
    w/o. consistency & 90.4 & 68.7 & 16.8 &  86.4 & 57.1 & 84.1 & 66.3 & 81.2 & 70.9 & 13.1 & 14.6 \\
    \bottomrule
  \end{tabular}
  \caption{AuGPT ablation study. The model version with the best ConvLab~2 success rate is chosen as our best model and dubbed AuGPT. Variants are denoted with their respective modifications compared to AuGPT: ``w/o.\ unlikelihood'' = unlikelihood loss was not used for training; ``w/o.\ clean'' uses all training samples as opposed to using only the ones consistent with the database; ``w/o.\ pre-training'' = the additional Taskmaster-1 and Schema-Guided datasets were not used for training; ``all auxiliary'' = using two additional auxiliary tasks (see the Method section for details); ``w/o.\ consistency'' = dialogue consistency task is not used; ``old consistency'' refers to the consistency task as defined by \citet{peng2020} (see the Method section for details). All variants submitted to the DSTC9 shared task are denoted “*”, the model chosen for DSTC9 human evaluation is denoted “\textdagger”.}
  \label{tab:ablation_comparison}
\end{table*}

\section{Results}
In this section, we first describe and discuss the quantitative results.
In the second part, we also perform a qualitative analysis of the model behavior.

\subsection{Comparison to State-of-the-Art on MultiWOZ}
Table \ref{tab:multiwoz_sota_comparison} shows a comparison between our methods and current state-of-the-art systems, which are described in the Related Work section.
Since \multiwozn has been released quite recently and some of the compared methods do not provide results with this version, we report results on both \multiwoz and MultiWOZ 2.1.
As we can see, AuGPT outperforms all other approaches in terms of the \emph{inform} and \emph{success} metrics. However, DAMD and LABES-S2S produce higher BLEU scores.
This would indicate better fluency of these models, however, one would need human evaluation to confidently claim that.
One possible reason for this behavior would be our removal of some training samples (see Data Preprocessing), which may have decreased the BLEU score.
Importantly, thanks to the higher \emph{success} metric, we can say that our model is better at providing all the necessary information in the responses.

Table~\ref{tab:multiwoz_convlab_comparison} shows a comparison with two other models in the ConvLab evaluation scheme with a simulated user. The compared systems were chosen because they both implement fully trainable end-to-end methods. Our system outperforms both compared systems by a wide margin. Our model is able to perform well not just in a single-turn response generation scenario, but over the course of the whole dialogue. As the example of DAMD shows, this is not always guaranteed.

\begin{table*}[ht]
    \centering
    \begin{tabular}{lcccccc}
      \toprule
      Method & Average Success & Success w/ DB & Success w/o DB & NLU score & Response appropriatenes & Turns \\
      \midrule
      Baseline & 69.6 & 56.8 & 82.4 & 4.34 & 4.18 & 18.5 \\
      Team1 (winner) & \textbf{74.8} & \textbf{70.2} & 79.4 & \textbf{4.54 }& \textbf{4.47} & 18.5 \\
      Team7 (ours) & 72.3 & 62.0 & \textbf{82.6} & 4.53 & 4.41 & \textbf{17.1} \\
      \bottomrule
    \end{tabular}
    \caption{Human evaluation results on the DSTC9 Track 2. We show a shortened version of the table, the full version is published online at \url{https://convlab.github.io}. Note that only 4 teams outperformed the Baseline according to the average success metric.}
    \label{tab:human}
\end{table*}


\subsection{Human Evaluation}

We show the DSTC9 human evaluation results in Table~\ref{tab:human}.
%The primary metric here was the \emph{Average success}.
Our model outperformed the baseline on the main average success metric (along with only half of other submissions).
We did not match the winning submission in terms of average success, but we achieved the best overall success rate based on human judgments (without checking consistency with the database).
This suggests that the biggest drawback of our model was the integration with database grounding.
This can be caused by our modified delexicalization, which makes it harder in some cases to match the database properly.
In addition, our model 
%However, our model performed well in the scenario when the database was not considered and 
needed the smallest number of turns overall to complete the evaluation dialogues.

We note that out of 5 submitted model variants in the DSTC9 competition (cf.~Table~\ref{tab:ablation_comparison}), only the model variant scoring best in the automatic evaluation was considered for human evaluation.
In our case, the chosen model variant did not include additional data pre-training, which could possibly improve performance in the human evaluation scenario.


\subsection{Qualitative Analysis}
\begin{example}
\setlength{\tabcolsep}{2pt}
\begin{mdframed}[style=ExampleFrame]
\begin{tabularx}{\textwidth}{rX}
\textbf{user:} & Can you tell me the entrance fee ? \\
\textbf{system:} & I don't have that information, but here is the phone number: 01223902112. \\
\textbf{user:} & I will need the entrance fee please .\\
\textbf{system:} & I'm sorry, I don't have that information.\\
\textbf{user:} & I just need to know how much the entrance fee is . \\
\textbf{system:} & The entrance fee is 4 pounds. \\
\end{tabularx}
\end{mdframed}
\caption{Dialogue sample with bad delexicalization.\label{ex:error1}}
\end{example}

We have analyzed the errors in interactions of our dialogue system and the ConvLab 2 user simulator. One of the errors we have noticed was the lack of grounding for some requested information. This is demonstrated in \exampleref{ex:error1}. The reason for this error is our modified delexicalization, which ignores several rare slots such as the entrance fee. 
We choose to ignore these slots since their annotation in the MultiWOZ set is largely inconsistent.
%We did it because the dialogues were not consistent with the database and often stated that the entrance was free even if it was not the case. 
We therefore were not able to delexicalize the entrance fee properly and decided to ignore it altogether, increasing our overall success rate.

\begin{example}
\setlength{\tabcolsep}{2pt}
\begin{mdframed}[style=ExampleFrame]
\begin{tabularx}{\textwidth}{rX}
\textbf{user:} & I also need a hotel. Could you also find me a place to stay with a cheap price ? [...] \\
\textbf{system:} & I have two guesthouses that meet your criteria. Would you like to stay in a guesthouse, or a guesthouse? \\
\textbf{user:} & Could you help me with a finding a guesthouse~? Please pick one for me. \\
\end{tabularx}
\end{mdframed}
% system: I have two guesthouses available. Do you have a preference for the number of stars or the number of rooms needed?
\caption{Dialogue sample with bad delexicalization.\label{ex:error2}}
\end{example}

Another quite rare error is shown in \exampleref{ex:error2}. The system responds with a question if the user wants to stay in ``a guesthouse or a guesthouse''. The reason is that the system has no access to the number of distinct accommodation types based on the database result counts only. It thus generated a response that seems appropriate, but does not fit the case where there is only one accommodation type.

We would like to state that these cases were specifically chosen after examining 100 dialogues to demonstrate the errors made by \textbf{AuGPT} system. We noticed that dialogue failures were caused more often by the ConvLab 2 user agent, which frequently used ungrammatical constructs and repeatedly asked for the same information.
One example of ConvLab 2 error was the case of the \textit{price range} slot, where the user agent upon reading the system response containing ``moderately priced'' used this value verbatim for this slot, while the correct value should have been ``moderate''.



\section{Ablation Study}
We tested many variants of our method with different combinations of our proposed system's components to evaluate their contributions. The results are presented in Table \ref{tab:ablation_comparison}.
Namely, we are interested in the following components:
\textbf{(1)} the unlikelihood loss, \textbf{(2)} the auxiliary tasks, \textbf{(3)} the data augmentation, \textbf{(4)} the modified consistency task and \textbf{(5)} unclean data filtering.

We can see that all proposed contributions which are a part of the final \textbf{AuGPT} system have a positive effect on the system performance with respect to the primary metrics.

We can see that removing either the pre-training or the back-translations decreases the BLEU score and, more importantly, the success rates. Furthermore, we notice the positive effect of using our improved consistency detection task over the one used in SOLOIST \cite{peng2020}, which in turn scores better than no consistency detection. 

Removing either the unlikelihood loss or training on all data as opposed to only “clean” samples clearly reduces performance.
However, we did not notice any increase in performance when the user intent prediction and system action prediction auxiliary tasks were used.
The reason for this behavior could be that the model learns to represent the actions well enough implicitly, without the need for these additional objectives.
However, these tasks are not a part of the final \textbf{AuGPT} model.

%One interesting aspect of the results is that the book rate did not correspond well with the success rate. This inconsistency might be caused by the ConvLab 2 evaluation itself, where the database used by the ConvLab 2 user agent expected an exact wording of the response returned by our system.

\section{Conclusions \& Future Work}
We present \textbf{AuGPT}, a dialogue modeling pipeline based on the pre-trained GPT-2 language model.
\textbf{AuGPT} uses modified training objectives and employs data augmentation to increase the diversity of generated utterances.
Our experiments show that the proposed approach performs better than state-of-the-art baselines in a multi-domain scenario on the MultiWOZ dataset.
We also run a series of ablation experiments to assess the individual contributions of the modifications.
According to our detailed ablation study, 
training data augmentation using back-translation via multiple languages and a modified auxiliary training objective for dialogue consistency detection are the features that contribute most to our system's performance.
Additionally, we perform a qualitative analysis of the outputs to give a better insight into our model behavior.

We participated in the DSTC9 shared task for multi-domain task-oriented end-to-end dialogue modeling.
Our submission placed third in the final evaluation while obtaining the best results in the \emph{success} metric without database grounding and achieving the shortest dialogues on average.

% In the future work, we'll focus on exploiting the additional data sources for pre-training.

In the future, we plan to construct a latent representation of the belief state and optimize it jointly with the language model. We will replace the deterministic lexicalization with a trainable alternative, and possibly even integrate the database module into the model. To improve the transfer to new domains, we will learn a domain embedding and optimize it jointly with the model, unifying all datasets.

%Our model placed third in the final evaluation run, where human subjects were employed to judge the system's quality.
%However, only the best model in the automated evaluation  got to the final run.
%Due to some uncertainty in the simulated automated evaluation, the selected model wasn't the variant that performed best in our experiments.
%We thus believe, that had this variant been selected, our human evaluation score would be even better.

%\subsection{Acknowledgments}
% Not submitted for anonymity
%\JK{TODO:Metacentrum}
%\VH{GAUK}

%Jonáš Kulhánek was supported by the European Regional Development Fund under the project Robotics for Industry 4.0 (reg.~no. CZ.02.1.01/0.0/0.0/15\_003/0000470).
\bibliography{bibliography}

\end{document}

% useful texts

The transformers architecture \cite{vaswani2017} enables us to train the auto-regressive language model fast, since we can compute the gradients in parallel as opposed to RNN-based seq2seq models \cite{sutskever2014,bahdanau_neural_2015}.
